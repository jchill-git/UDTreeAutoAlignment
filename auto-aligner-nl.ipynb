{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_nl = \"soms vraagt ze me waarom ik haar vader Harold noemde\"\n",
    "sent_en = \"sometimes she asks me why I used to call her father Harold\"\n",
    "sent_fr = \"parfois elle me demande pourquoi j'appelais son père Harold\"\n",
    "sent_fi = \"joskus hän kysyy minulta, miksi kutsuin hänen isäänsä Haroldiksi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_torch_default_device' from 'thinc.api' (/Users/tanaynistala/.conda/NLP/lib/python3.10/site-packages/thinc/api.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# !python -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# !python -m spacy download nl_core_news_sm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# !python -m spacy download fr_core_news_sm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# !python -m spacy download fi_core_news_sm\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m nlp_en \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m nlp_nl \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mnl_core_news_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m nlp_fr \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mfr_core_news_sm\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m     53\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/util.py:420\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m get_lang_class(name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mblank:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))()\n\u001b[1;32m    419\u001b[0m \u001b[39mif\u001b[39;00m is_package(name):  \u001b[39m# installed as package\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_package(name, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m Path(name)\u001b[39m.\u001b[39mexists():  \u001b[39m# path to model data directory\u001b[39;00m\n\u001b[1;32m    422\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/util.py:453\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \n\u001b[1;32m    440\u001b[0m \u001b[39mname (str): The package name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(name)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mload(vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig)\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/en_core_web_sm/__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moverrides):\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m load_model_from_init_py(\u001b[39m__file__\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moverrides)\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/util.py:619\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_path\u001b[39m.\u001b[39mexists():\n\u001b[1;32m    618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE052\u001b[39m.\u001b[39mformat(path\u001b[39m=\u001b[39mdata_path))\n\u001b[0;32m--> 619\u001b[0m \u001b[39mreturn\u001b[39;00m load_model_from_path(\n\u001b[1;32m    620\u001b[0m     data_path,\n\u001b[1;32m    621\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    622\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    623\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    624\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    625\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/util.py:488\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    486\u001b[0m overrides \u001b[39m=\u001b[39m dict_to_dot(config)\n\u001b[1;32m    487\u001b[0m config \u001b[39m=\u001b[39m load_config(config_path, overrides\u001b[39m=\u001b[39moverrides)\n\u001b[0;32m--> 488\u001b[0m nlp \u001b[39m=\u001b[39m load_model_from_config(\n\u001b[1;32m    489\u001b[0m     config, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, meta\u001b[39m=\u001b[39;49mmeta\n\u001b[1;32m    490\u001b[0m )\n\u001b[1;32m    491\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\u001b[39m.\u001b[39mfrom_disk(model_path, exclude\u001b[39m=\u001b[39mexclude, overrides\u001b[39m=\u001b[39moverrides)\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/util.py:528\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[0;34m(config, meta, vocab, disable, exclude, auto_fill, validate)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[1;32m    527\u001b[0m lang_cls \u001b[39m=\u001b[39m get_lang_class(nlp_config[\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 528\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls\u001b[39m.\u001b[39;49mfrom_config(\n\u001b[1;32m    529\u001b[0m     config,\n\u001b[1;32m    530\u001b[0m     vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[1;32m    531\u001b[0m     disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[1;32m    532\u001b[0m     exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[1;32m    533\u001b[0m     auto_fill\u001b[39m=\u001b[39;49mauto_fill,\n\u001b[1;32m    534\u001b[0m     validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    535\u001b[0m     meta\u001b[39m=\u001b[39;49mmeta,\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    537\u001b[0m \u001b[39mreturn\u001b[39;00m nlp\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/language.py:1779\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[0;34m(cls, config, vocab, disable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[1;32m   1773\u001b[0m warn_if_jupyter_cupy()\n\u001b[1;32m   1775\u001b[0m \u001b[39m# Note that we don't load vectors here, instead they get loaded explicitly\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m \u001b[39m# inside stuff like the spacy train function. If we loaded them here,\u001b[39;00m\n\u001b[1;32m   1777\u001b[0m \u001b[39m# then we would load them twice at runtime: once when we make from config,\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[39m# and then again when we load from disk.\u001b[39;00m\n\u001b[0;32m-> 1779\u001b[0m nlp \u001b[39m=\u001b[39m lang_cls(vocab\u001b[39m=\u001b[39;49mvocab, create_tokenizer\u001b[39m=\u001b[39;49mcreate_tokenizer, meta\u001b[39m=\u001b[39;49mmeta)\n\u001b[1;32m   1780\u001b[0m \u001b[39mif\u001b[39;00m after_creation \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1781\u001b[0m     nlp \u001b[39m=\u001b[39m after_creation(nlp)\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy/language.py:162\u001b[0m, in \u001b[0;36mLanguage.__init__\u001b[0;34m(self, vocab, max_length, meta, create_tokenizer, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39m\"\"\"Initialise a Language object.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[39mvocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mDOCS: https://spacy.io/api/language#init\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m# We're only calling this to import all factories provided via entry\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m# points. The factory decorator applied to these functions takes care\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m# of the rest.\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m util\u001b[39m.\u001b[39;49mregistry\u001b[39m.\u001b[39;49m_entry_point_factories\u001b[39m.\u001b[39;49mget_all()\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config \u001b[39m=\u001b[39m DEFAULT_CONFIG\u001b[39m.\u001b[39mmerge(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault_config)\n\u001b[1;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_meta \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(meta)\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/catalogue/__init__.py:109\u001b[0m, in \u001b[0;36mRegistry.get_all\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[1;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_points:\n\u001b[0;32m--> 109\u001b[0m     result\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_entry_points())\n\u001b[1;32m    110\u001b[0m \u001b[39mfor\u001b[39;00m keys, value \u001b[39min\u001b[39;00m REGISTRY\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamespace) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(keys) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mall\u001b[39m(\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamespace[i] \u001b[39m==\u001b[39m keys[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamespace))\n\u001b[1;32m    113\u001b[0m     ):\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/catalogue/__init__.py:124\u001b[0m, in \u001b[0;36mRegistry.get_entry_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[1;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m entry_point \u001b[39min\u001b[39;00m AVAILABLE_ENTRY_POINTS\u001b[39m.\u001b[39mget(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point_namespace, []):\n\u001b[0;32m--> 124\u001b[0m     result[entry_point\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m entry_point\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    125\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/importlib/metadata/__init__.py:171\u001b[0m, in \u001b[0;36mEntryPoint.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39m\"\"\"Load the entry point from its definition. If only a module\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mis indicated by the value, return that module. Otherwise,\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn the named object.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m match \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpattern\u001b[39m.\u001b[39mmatch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue)\n\u001b[0;32m--> 171\u001b[0m module \u001b[39m=\u001b[39m import_module(match\u001b[39m.\u001b[39;49mgroup(\u001b[39m'\u001b[39;49m\u001b[39mmodule\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m    172\u001b[0m attrs \u001b[39m=\u001b[39m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (match\u001b[39m.\u001b[39mgroup(\u001b[39m'\u001b[39m\u001b[39mattr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m functools\u001b[39m.\u001b[39mreduce(\u001b[39mgetattr\u001b[39m, attrs, module)\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:992\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_wrap/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mabout\u001b[39;00m \u001b[39mimport\u001b[39;00m __documentation__, __download_url__, __title__  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39marchitectures\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     create_SequenceClassificationTransformerModel_v1,\n\u001b[1;32m      4\u001b[0m     create_TokenClassificationTransformerModel_v1,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpipeline_component_seq_clf\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     SequenceClassificationTransformer,\n\u001b[1;32m      8\u001b[0m     make_sequence_classification_transformer,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpipeline_component_tok_clf\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     TokenClassificationTransformer,\n\u001b[1;32m     12\u001b[0m     make_token_classification_transformer,\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_wrap/architectures.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, List\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy_transformers\u001b[39;00m \u001b[39mimport\u001b[39;00m FullTransformerBatch\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy_transformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m registry\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_transformers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m architectures\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m annotation_setters\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m span_getters\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_transformers/architectures.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m Ragged, Floats2d\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m TransformerModel, TransformerListener\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m trfs2arrays, split_trf_batch\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m registry\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_transformers/layers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlistener\u001b[39;00m \u001b[39mimport\u001b[39;00m TransformerListener\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtransformer_model\u001b[39;00m \u001b[39mimport\u001b[39;00m TransformerModel\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msplit_trf\u001b[39;00m \u001b[39mimport\u001b[39;00m split_trf_batch\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_transformers/layers/listener.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m Errors\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Doc\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_classes\u001b[39;00m \u001b[39mimport\u001b[39;00m TransformerData\n\u001b[1;32m      8\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTransformerListener\u001b[39;00m(Model):\n\u001b[1;32m      9\u001b[0m     \u001b[39m\"\"\"A layer that gets fed its answers from an upstream connection,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m    for instance from a component earlier in the pipeline.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_transformers/data_classes.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtokens\u001b[39;00m \u001b[39mimport\u001b[39;00m Span\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrsly\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m transpose_list\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39malign\u001b[39;00m \u001b[39mimport\u001b[39;00m get_token_positions\n\u001b[1;32m     17\u001b[0m \u001b[39m@dataclass\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mWordpieceBatch\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/NLP/lib/python3.10/site-packages/spacy_transformers/util.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcatalogue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m registry\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthinc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m \u001b[39mimport\u001b[39;00m get_torch_default_device\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcuda\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtempfile\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_torch_default_device' from 'thinc.api' (/Users/tanaynistala/.conda/NLP/lib/python3.10/site-packages/thinc/api.py)"
     ]
    }
   ],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download nl_core_news_sm\n",
    "# !python -m spacy download fr_core_news_sm\n",
    "# !python -m spacy download fi_core_news_sm\n",
    "\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_nl = spacy.load(\"nl_core_news_sm\")\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "nlp_fi = spacy.load(\"fi_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_en = nlp_en(sent_en)\n",
    "doc_nl = nlp_nl(sent_nl)\n",
    "doc_fr = nlp_fr(sent_fr)\n",
    "doc_fi = nlp_fi(sent_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:\n",
      "sometimes\tadvmod\tasks\tADV\t[]\n",
      "she\tnsubj\tasks\tPRON\t[]\n",
      "asks\tROOT\tasks\tVERB\t[sometimes, she, me, used]\n",
      "me\tdobj\tasks\tPRON\t[]\n",
      "why\tadvmod\tused\tSCONJ\t[]\n",
      "I\tnsubj\tused\tPRON\t[]\n",
      "used\tccomp\tasks\tVERB\t[why, I, call]\n",
      "to\taux\tcall\tPART\t[]\n",
      "call\txcomp\tused\tVERB\t[to, father, Harold]\n",
      "her\tposs\tfather\tPRON\t[]\n",
      "father\tdobj\tcall\tNOUN\t[her]\n",
      "Harold\toprd\tcall\tPROPN\t[]\n",
      "\n",
      "Dutch:\n",
      "soms\tadvmod\tvraagt\tADV\t[]\n",
      "vraagt\tROOT\tvraagt\tVERB\t[soms, ze, me, noemde]\n",
      "ze\tnsubj\tvraagt\tPRON\t[]\n",
      "me\tiobj\tvraagt\tPRON\t[]\n",
      "waarom\tadvmod\tnoemde\tADV\t[]\n",
      "ik\tnsubj\tnoemde\tPRON\t[]\n",
      "haar\tnmod:poss\tvader\tPRON\t[]\n",
      "vader\tobj\tnoemde\tNOUN\t[haar, Harold]\n",
      "Harold\tappos\tvader\tPROPN\t[]\n",
      "noemde\tccomp\tvraagt\tVERB\t[waarom, ik, vader]\n",
      "\n",
      "French:\n",
      "parfois\tadvmod\tdemande\tADV\t[]\n",
      "elle\tnsubj\tdemande\tPRON\t[]\n",
      "me\tiobj\tdemande\tPRON\t[]\n",
      "demande\tROOT\tdemande\tVERB\t[parfois, elle, me, appelais]\n",
      "pourquoi\tadvmod\tappelais\tADV\t[]\n",
      "j'\tnsubj\tappelais\tPRON\t[]\n",
      "appelais\tccomp\tdemande\tVERB\t[pourquoi, j', père]\n",
      "son\tdet\tpère\tDET\t[]\n",
      "père\tobj\tappelais\tNOUN\t[son, Harold]\n",
      "Harold\tflat:name\tpère\tPROPN\t[]\n",
      "\n",
      "Finnish:\n",
      "joskus\tadvmod\tkysyy\tADV\t[]\n",
      "hän\tnsubj\tkysyy\tPRON\t[]\n",
      "kysyy\tROOT\tkysyy\tVERB\t[joskus, hän, minulta, kutsuin]\n",
      "minulta\tobl\tkysyy\tPRON\t[]\n",
      ",\tpunct\tkutsuin\tPUNCT\t[]\n",
      "miksi\tadvmod\tkutsuin\tADV\t[]\n",
      "kutsuin\tccomp\tkysyy\tVERB\t[,, miksi, isäänsä, Haroldiksi]\n",
      "hänen\tnmod:poss\tisäänsä\tPRON\t[]\n",
      "isäänsä\tobj\tkutsuin\tNOUN\t[hänen]\n",
      "Haroldiksi\tobl\tkutsuin\tPROPN\t[]\n"
     ]
    }
   ],
   "source": [
    "print(\"English:\")\n",
    "ud_tree_en = dict()\n",
    "for token in doc_en:\n",
    "    print(token.text, token.dep_, token.head.text, token.pos_,\n",
    "            [child for child in token.children], sep=\"\\t\")\n",
    "    \n",
    "    ud_tree_en[token.text] = [token.dep_, [child for child in token.children]]\n",
    "\n",
    "print(\"\\nDutch:\")\n",
    "ud_tree_nl = dict()\n",
    "for token in doc_nl:\n",
    "    print(token.text, token.dep_, token.head.text, token.pos_,\n",
    "        [child for child in token.children], sep=\"\\t\")\n",
    "    \n",
    "    ud_tree_nl[token.text] = [token.dep_, [child for child in token.children]]\n",
    "\n",
    "print(\"\\nFrench:\")\n",
    "ud_tree_fr = dict()\n",
    "for token in doc_fr:\n",
    "    print(token.text, token.dep_, token.head.text, token.pos_,\n",
    "        [child for child in token.children], sep=\"\\t\")\n",
    "\n",
    "    ud_tree_fr[token.text] = [token.dep_, [child for child in token.children]]\n",
    "\n",
    "print(\"\\nFinnish:\")\n",
    "ud_tree_fi = dict()\n",
    "for token in doc_fi:\n",
    "    print(token.text, token.dep_, token.head.text, token.pos_,\n",
    "        [child for child in token.children], sep=\"\\t\")\n",
    "    \n",
    "    ud_tree_fi[token.text] = [token.dep_, [child for child in token.children]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English UD tree:\n",
      "{'sometimes': ['advmod', []], 'she': ['nsubj', []], 'asks': ['ROOT', [sometimes, she, me, used]], 'me': ['dobj', []], 'why': ['advmod', []], 'I': ['nsubj', []], 'used': ['ccomp', [why, I, call]], 'to': ['aux', []], 'call': ['xcomp', [to, father, Harold]], 'her': ['poss', []], 'father': ['dobj', [her]], 'Harold': ['oprd', []]}\n",
      "\n",
      "Dutch UD tree:\n",
      "{'soms': ['advmod', []], 'vraagt': ['ROOT', [soms, ze, me, noemde]], 'ze': ['nsubj', []], 'me': ['iobj', []], 'waarom': ['advmod', []], 'ik': ['nsubj', []], 'haar': ['nmod:poss', []], 'vader': ['obj', [haar, Harold]], 'Harold': ['appos', []], 'noemde': ['ccomp', [waarom, ik, vader]]}\n",
      "\n",
      "French UD tree:\n",
      "{'parfois': ['advmod', []], 'elle': ['nsubj', []], 'me': ['iobj', []], 'demande': ['ROOT', [parfois, elle, me, appelais]], 'pourquoi': ['advmod', []], \"j'\": ['nsubj', []], 'appelais': ['ccomp', [pourquoi, j', père]], 'son': ['det', []], 'père': ['obj', [son, Harold]], 'Harold': ['flat:name', []]}\n",
      "\n",
      "Finnish UD tree:\n",
      "{'joskus': ['advmod', []], 'hän': ['nsubj', []], 'kysyy': ['ROOT', [joskus, hän, minulta, kutsuin]], 'minulta': ['obl', []], ',': ['punct', []], 'miksi': ['advmod', []], 'kutsuin': ['ccomp', [,, miksi, isäänsä, Haroldiksi]], 'hänen': ['nmod:poss', []], 'isäänsä': ['obj', [hänen]], 'Haroldiksi': ['obl', []]}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEnglish UD tree:\")\n",
    "print(ud_tree_en)\n",
    "\n",
    "print(\"\\nDutch UD tree:\")\n",
    "print(ud_tree_nl)\n",
    "\n",
    "print(\"\\nFrench UD tree:\")\n",
    "print(ud_tree_fr)\n",
    "\n",
    "print(\"\\nFinnish UD tree:\")\n",
    "print(ud_tree_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English root: asks\n",
      "Dutch root: vraagt\n",
      "French root: demande\n",
      "Finnish root: kysyy\n"
     ]
    }
   ],
   "source": [
    "root_en = [key for key in ud_tree_en.keys() if ud_tree_en[key][0] == \"ROOT\"][0]\n",
    "root_nl = [key for key in ud_tree_nl.keys() if ud_tree_nl[key][0] == \"ROOT\"][0]\n",
    "root_fr = [key for key in ud_tree_fr.keys() if ud_tree_fr[key][0] == \"ROOT\"][0]\n",
    "root_fi = [key for key in ud_tree_fi.keys() if ud_tree_fi[key][0] == \"ROOT\"][0]\n",
    "\n",
    "print(\"\\nEnglish root:\", root_en)\n",
    "print(\"Dutch root:\", root_nl)\n",
    "print(\"French root:\", root_fr)\n",
    "print(\"Finnish root:\", root_fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_node(root_en, root_nl, parent_en=None, parent_nl=None):\n",
    "    matches = list()\n",
    "\n",
    "    pos_en, children_en = ud_tree_en[root_en]\n",
    "    pos_nl, children_nl = ud_tree_fi[root_nl]\n",
    "\n",
    "    # Ignore/match across direct/indirect objects\n",
    "    if pos_en == \"iobj\" or pos_en == \"dobj\":\n",
    "        pos_en = \"obj\"\n",
    "    if pos_nl == \"iobj\" or pos_nl == \"dobj\":\n",
    "        pos_nl = \"obj\"\n",
    "\n",
    "    # Ignore/match across modifiers\n",
    "    pos_en = pos_en.split(\":\")[-1]\n",
    "    pos_nl = pos_nl.split(\":\")[-1]\n",
    "\n",
    "    if pos_en == pos_nl and pos_en != \"punct\" and pos_nl != \"punct\":\n",
    "        matches.append((root_en, root_nl))\n",
    "        print(\"Match:\", root_en, root_nl, \"for pos\", pos_en)\n",
    "\n",
    "    for child_en in children_en:\n",
    "        for child_nl in children_nl:\n",
    "            matches.extend(check_node(child_en.text, child_nl.text, parent_en=root_en, parent_nl=root_nl))\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: asks kysyy for pos ROOT\n",
      "Match: sometimes joskus for pos advmod\n",
      "Match: she hän for pos nsubj\n",
      "Match: used kutsuin for pos ccomp\n",
      "Match: why miksi for pos advmod\n",
      "\n",
      "Matches:\n",
      "[('asks', 'kysyy'), ('sometimes', 'joskus'), ('she', 'hän'), ('used', 'kutsuin'), ('why', 'miksi')]\n"
     ]
    }
   ],
   "source": [
    "matches = check_node(root_en, root_fi)\n",
    "print(\"\\nMatches:\")\n",
    "print(matches)\n",
    "\n",
    "matched_en = [match[0] for match in matches]\n",
    "matched_fi = [match[1] for match in matches]\n",
    "\n",
    "unmatched_en = [token for token in doc_en if token.text not in matched_en]\n",
    "unmatched_nl = [token for token in doc_fi if token.text not in matched_fi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me\tdobj\tasks\tPRON\t[]\n",
      "I\tnsubj\tused\tPRON\t[]\n",
      "to\taux\tcall\tPART\t[]\n",
      "call\txcomp\tused\tVERB\t[to, father, Harold]\n",
      "her\tposs\tfather\tPRON\t[]\n",
      "father\tdobj\tcall\tNOUN\t[her]\n",
      "Harold\toprd\tcall\tPROPN\t[]\n",
      "\n",
      "minulta\tobl\tkysyy\tPRON\t[]\n",
      ",\tpunct\tkutsuin\tPUNCT\t[]\n",
      "hänen\tnmod:poss\tisäänsä\tPRON\t[]\n",
      "isäänsä\tobj\tkutsuin\tNOUN\t[hänen]\n",
      "Haroldiksi\tobl\tkutsuin\tPROPN\t[]\n"
     ]
    }
   ],
   "source": [
    "for token in unmatched_en:\n",
    "    print(token.text, token.dep_, token.head.text, token.pos_,\n",
    "            [child for child in token.children], sep=\"\\t\")\n",
    "    \n",
    "print()\n",
    "\n",
    "for token in unmatched_nl:\n",
    "    print(token.text, token.dep_, token.head.text, token.pos_,\n",
    "            [child for child in token.children], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me obj\n",
      "I nsubj\n",
      "to aux\n",
      "call xcomp\n",
      "her poss\n",
      "father obj\n",
      "Harold oprd\n",
      "minulta obl\n",
      ", punct\n",
      "hänen poss\n",
      "isäänsä obj\n",
      "Haroldiksi obl\n"
     ]
    }
   ],
   "source": [
    "for token in [*unmatched_en, *unmatched_nl]:\n",
    "    # Ignore/match across direct/indirect objects\n",
    "    if token.dep_ == \"iobj\" or token.dep_ == \"dobj\":\n",
    "        token.dep_ = \"obj\"\n",
    "\n",
    "    # Ignore/match across modifiers\n",
    "    token.dep_ = token.dep_.split(\":\")[-1]\n",
    "\n",
    "    print(token.text, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: me isäänsä for dep obj\n",
      "Match: her hänen for dep poss\n",
      "Match: father isäänsä for dep obj\n"
     ]
    }
   ],
   "source": [
    "# Match remaining tokens\n",
    "for token_en in unmatched_en:\n",
    "    for token_nl in unmatched_nl:\n",
    "        if token_en.dep_ == token_nl.dep_:\n",
    "            print(\"Match:\", token_en.text, token_nl.text, \"for dep\", token_en.dep_)\n",
    "            matches.append((token_en.text, token_nl.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asks', 'kysyy'),\n",
       " ('sometimes', 'joskus'),\n",
       " ('she', 'hän'),\n",
       " ('used', 'kutsuin'),\n",
       " ('why', 'miksi'),\n",
       " ('me', 'isäänsä'),\n",
       " ('her', 'hänen'),\n",
       " ('father', 'isäänsä')]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
